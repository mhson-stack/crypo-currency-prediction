{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Flatten, Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import pickle\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "\n",
    "from database import export_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Datast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting BTC\n"
     ]
    }
   ],
   "source": [
    "btc_df = export_db('BTC')\n",
    "train_dates = btc_df[\"OpenTime\"]\n",
    "train = btc_df['Open']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.88104107],\n",
       "       [-0.87830276],\n",
       "       [-0.8770617 ],\n",
       "       ...,\n",
       "       [ 0.02475432],\n",
       "       [ 0.0256507 ],\n",
       "       [ 0.02451374]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.values.reshape(-1, 1)\n",
    "scaler = StandardScaler() \n",
    "train_transformed = scaler.fit_transform(train)\n",
    "train_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Spli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(train_transformed) * 0.66)\n",
    "train, test = train_transformed[:train_size, :], train_transformed[train_size:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sequences(dataset, seq_size=1):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(dataset)-seq_size-1):\n",
    "        #print(i)\n",
    "        window = dataset[i:(i+seq_size), 0]\n",
    "        x.append(window)\n",
    "        y.append(dataset[i+seq_size, 0])\n",
    "        \n",
    "    return np.array(x),np.array(y)\n",
    "    \n",
    "\n",
    "seq_size = 100  # Number of time steps to look back \n",
    "#Larger sequences (look further back) may improve forecasting.\n",
    "\n",
    "trainX, trainY = to_sequences(train, seq_size)\n",
    "testX, testY = to_sequences(test, seq_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28921, 1, 100)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dw/94thms1n5817tsrqwqcs_3rr0000gn/T/ipykernel_48830/1858071613.py:11: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(generator,epochs=100,verbose=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4835/4835 [==============================] - 56s 11ms/step - loss: 0.0025\n",
      "Epoch 2/100\n",
      "4835/4835 [==============================] - 55s 11ms/step - loss: 7.4386e-04\n",
      "Epoch 3/100\n",
      "4835/4835 [==============================] - 51s 11ms/step - loss: 4.8898e-04\n",
      "Epoch 4/100\n",
      "4835/4835 [==============================] - 58s 12ms/step - loss: 4.1859e-04\n",
      "Epoch 5/100\n",
      "4835/4835 [==============================] - 59s 12ms/step - loss: 3.7208e-04\n",
      "Epoch 6/100\n",
      "4835/4835 [==============================] - 60s 12ms/step - loss: 3.7361e-04\n",
      "Epoch 7/100\n",
      "4835/4835 [==============================] - 66s 14ms/step - loss: 3.6391e-04\n",
      "Epoch 8/100\n",
      "4835/4835 [==============================] - 81s 17ms/step - loss: 3.4480e-04\n",
      "Epoch 9/100\n",
      "4835/4835 [==============================] - 88s 18ms/step - loss: 3.3954e-04\n",
      "Epoch 10/100\n",
      "4835/4835 [==============================] - 96s 20ms/step - loss: 3.3114e-04\n",
      "Epoch 11/100\n",
      "4835/4835 [==============================] - 92s 19ms/step - loss: 3.2246e-04\n",
      "Epoch 12/100\n",
      "4835/4835 [==============================] - 105s 22ms/step - loss: 3.1132e-04\n",
      "Epoch 13/100\n",
      "4835/4835 [==============================] - 92s 19ms/step - loss: 3.3398e-04\n",
      "Epoch 14/100\n",
      "4835/4835 [==============================] - 98s 20ms/step - loss: 3.1709e-04\n",
      "Epoch 15/100\n",
      "4835/4835 [==============================] - 102s 21ms/step - loss: 3.2297e-04\n",
      "Epoch 16/100\n",
      "4835/4835 [==============================] - 87s 18ms/step - loss: 3.2016e-04\n",
      "Epoch 17/100\n",
      "4835/4835 [==============================] - 85s 18ms/step - loss: 3.1412e-04\n",
      "Epoch 18/100\n",
      "4835/4835 [==============================] - 79s 16ms/step - loss: 3.1079e-04\n",
      "Epoch 19/100\n",
      "4835/4835 [==============================] - 79s 16ms/step - loss: 3.1386e-04\n",
      "Epoch 20/100\n",
      "4835/4835 [==============================] - 79s 16ms/step - loss: 3.2128e-04\n",
      "Epoch 21/100\n",
      "4835/4835 [==============================] - 78s 16ms/step - loss: 3.1888e-04\n",
      "Epoch 22/100\n",
      "4835/4835 [==============================] - 78s 16ms/step - loss: 3.1158e-04\n",
      "Epoch 23/100\n",
      "4835/4835 [==============================] - 82s 17ms/step - loss: 3.0796e-04\n",
      "Epoch 24/100\n",
      "4835/4835 [==============================] - 79s 16ms/step - loss: 3.0586e-04\n",
      "Epoch 25/100\n",
      "4835/4835 [==============================] - 83s 17ms/step - loss: 3.0088e-04\n",
      "Epoch 26/100\n",
      "4835/4835 [==============================] - 76s 16ms/step - loss: 2.9716e-04\n",
      "Epoch 27/100\n",
      "4835/4835 [==============================] - 95s 20ms/step - loss: 3.0659e-04\n",
      "Epoch 28/100\n",
      "4835/4835 [==============================] - 104s 21ms/step - loss: 3.0420e-04\n",
      "Epoch 29/100\n",
      "4835/4835 [==============================] - 100s 21ms/step - loss: 2.8621e-04\n",
      "Epoch 30/100\n",
      "4835/4835 [==============================] - 98s 20ms/step - loss: 2.9300e-04\n",
      "Epoch 31/100\n",
      "4835/4835 [==============================] - 81s 17ms/step - loss: 3.0792e-04\n",
      "Epoch 32/100\n",
      "4835/4835 [==============================] - 84s 17ms/step - loss: 3.0237e-04\n",
      "Epoch 33/100\n",
      "4835/4835 [==============================] - 65s 13ms/step - loss: 2.8821e-04\n",
      "Epoch 34/100\n",
      "4835/4835 [==============================] - 59s 12ms/step - loss: 3.0636e-04\n",
      "Epoch 35/100\n",
      "4835/4835 [==============================] - 59s 12ms/step - loss: 3.0084e-04\n",
      "Epoch 36/100\n",
      "4835/4835 [==============================] - 59s 12ms/step - loss: 2.9610e-04\n",
      "Epoch 37/100\n",
      "4835/4835 [==============================] - 57s 12ms/step - loss: 2.9754e-04\n",
      "Epoch 38/100\n",
      "4835/4835 [==============================] - 66s 14ms/step - loss: 2.9679e-04\n",
      "Epoch 39/100\n",
      "4835/4835 [==============================] - 106s 22ms/step - loss: 2.9317e-04\n",
      "Epoch 40/100\n",
      "4835/4835 [==============================] - 88s 18ms/step - loss: 2.9833e-04\n",
      "Epoch 41/100\n",
      "4835/4835 [==============================] - 88s 18ms/step - loss: 2.9686e-04\n",
      "Epoch 42/100\n",
      "4835/4835 [==============================] - 57s 12ms/step - loss: 2.9556e-04\n",
      "Epoch 43/100\n",
      "4835/4835 [==============================] - 64s 13ms/step - loss: 2.9223e-04\n",
      "Epoch 44/100\n",
      "4835/4835 [==============================] - 79s 16ms/step - loss: 3.0157e-04\n",
      "Epoch 45/100\n",
      "4835/4835 [==============================] - 84s 17ms/step - loss: 2.9920e-04\n",
      "Epoch 46/100\n",
      "4835/4835 [==============================] - 74s 15ms/step - loss: 2.9611e-04\n",
      "Epoch 47/100\n",
      "4835/4835 [==============================] - 78s 16ms/step - loss: 2.9333e-04\n",
      "Epoch 48/100\n",
      "4835/4835 [==============================] - 69s 14ms/step - loss: 2.9708e-04\n",
      "Epoch 49/100\n",
      "4835/4835 [==============================] - 74s 15ms/step - loss: 2.8803e-04\n",
      "Epoch 50/100\n",
      "4835/4835 [==============================] - 92s 19ms/step - loss: 2.9701e-04\n",
      "Epoch 51/100\n",
      "4835/4835 [==============================] - 89s 18ms/step - loss: 2.8687e-04\n",
      "Epoch 52/100\n",
      "4835/4835 [==============================] - 89s 18ms/step - loss: 2.8707e-04\n",
      "Epoch 53/100\n",
      "4835/4835 [==============================] - 94s 20ms/step - loss: 3.0133e-04\n",
      "Epoch 54/100\n",
      "4835/4835 [==============================] - 96s 20ms/step - loss: 2.8698e-04\n",
      "Epoch 55/100\n",
      "4835/4835 [==============================] - 90s 19ms/step - loss: 2.9240e-04\n",
      "Epoch 56/100\n",
      "4835/4835 [==============================] - 83s 17ms/step - loss: 2.8897e-04\n",
      "Epoch 57/100\n",
      "4835/4835 [==============================] - 77s 16ms/step - loss: 2.9128e-04\n",
      "Epoch 58/100\n",
      "4835/4835 [==============================] - 61s 13ms/step - loss: 2.9185e-04\n",
      "Epoch 59/100\n",
      "4835/4835 [==============================] - 57s 12ms/step - loss: 2.8973e-04\n",
      "Epoch 60/100\n",
      "4835/4835 [==============================] - 58s 12ms/step - loss: 2.9896e-04\n",
      "Epoch 61/100\n",
      "4835/4835 [==============================] - 56s 12ms/step - loss: 2.8451e-04\n",
      "Epoch 62/100\n",
      "4835/4835 [==============================] - 57s 12ms/step - loss: 2.8766e-04\n",
      "Epoch 63/100\n",
      "4835/4835 [==============================] - 57s 12ms/step - loss: 2.8794e-04\n",
      "Epoch 64/100\n",
      "4835/4835 [==============================] - 67s 14ms/step - loss: 2.9293e-04\n",
      "Epoch 65/100\n",
      "4835/4835 [==============================] - 96s 20ms/step - loss: 2.8453e-04\n",
      "Epoch 66/100\n",
      "4835/4835 [==============================] - 72s 15ms/step - loss: 2.9599e-04\n",
      "Epoch 67/100\n",
      "4835/4835 [==============================] - 65s 13ms/step - loss: 2.8611e-04\n",
      "Epoch 68/100\n",
      "4835/4835 [==============================] - 64s 13ms/step - loss: 2.9055e-04\n",
      "Epoch 69/100\n",
      "4835/4835 [==============================] - 64s 13ms/step - loss: 2.9060e-04\n",
      "Epoch 70/100\n",
      "4835/4835 [==============================] - 65s 13ms/step - loss: 2.9297e-04\n",
      "Epoch 71/100\n",
      "4835/4835 [==============================] - 65s 13ms/step - loss: 2.9690e-04\n",
      "Epoch 72/100\n",
      "4835/4835 [==============================] - 65s 13ms/step - loss: 2.9955e-04\n",
      "Epoch 73/100\n",
      "4835/4835 [==============================] - 71s 15ms/step - loss: 2.8326e-04\n",
      "Epoch 74/100\n",
      "4835/4835 [==============================] - 110s 23ms/step - loss: 2.8547e-04\n",
      "Epoch 75/100\n",
      "4835/4835 [==============================] - 99s 20ms/step - loss: 2.9141e-04\n",
      "Epoch 76/100\n",
      "4835/4835 [==============================] - 72s 15ms/step - loss: 2.8775e-04\n",
      "Epoch 77/100\n",
      "4835/4835 [==============================] - 76s 16ms/step - loss: 2.8568e-04\n",
      "Epoch 78/100\n",
      "4835/4835 [==============================] - 66s 14ms/step - loss: 2.9677e-04\n",
      "Epoch 79/100\n",
      "4835/4835 [==============================] - 87s 18ms/step - loss: 2.9665e-04\n",
      "Epoch 80/100\n",
      "4835/4835 [==============================] - 83s 17ms/step - loss: 2.8879e-04\n",
      "Epoch 81/100\n",
      "4835/4835 [==============================] - 82s 17ms/step - loss: 2.8330e-04\n",
      "Epoch 82/100\n",
      "4835/4835 [==============================] - 62s 13ms/step - loss: 2.8786e-04\n",
      "Epoch 83/100\n",
      "4835/4835 [==============================] - 60s 12ms/step - loss: 2.9249e-04\n",
      "Epoch 84/100\n",
      "4835/4835 [==============================] - 69s 14ms/step - loss: 2.8701e-04\n",
      "Epoch 85/100\n",
      "4835/4835 [==============================] - 74s 15ms/step - loss: 2.8787e-04\n",
      "Epoch 86/100\n",
      "4835/4835 [==============================] - 80s 17ms/step - loss: 2.7982e-04\n",
      "Epoch 87/100\n",
      "4835/4835 [==============================] - 74s 15ms/step - loss: 2.9079e-04\n",
      "Epoch 88/100\n",
      "4835/4835 [==============================] - 72s 15ms/step - loss: 2.8505e-04\n",
      "Epoch 89/100\n",
      "4835/4835 [==============================] - 73s 15ms/step - loss: 2.8647e-04\n",
      "Epoch 90/100\n",
      "4835/4835 [==============================] - 53s 11ms/step - loss: 2.8330e-04\n",
      "Epoch 91/100\n",
      "4835/4835 [==============================] - 102s 21ms/step - loss: 2.8616e-04\n",
      "Epoch 92/100\n",
      "4835/4835 [==============================] - 99s 21ms/step - loss: 2.8424e-04\n",
      "Epoch 93/100\n",
      "4835/4835 [==============================] - 78s 16ms/step - loss: 2.8665e-04\n",
      "Epoch 94/100\n",
      "4835/4835 [==============================] - 76s 16ms/step - loss: 2.8995e-04\n",
      "Epoch 95/100\n",
      "4835/4835 [==============================] - 71s 15ms/step - loss: 2.9388e-04\n",
      "Epoch 96/100\n",
      "4835/4835 [==============================] - 81s 17ms/step - loss: 2.8903e-04\n",
      "Epoch 97/100\n",
      "4835/4835 [==============================] - 77s 16ms/step - loss: 2.8618e-04\n",
      "Epoch 98/100\n",
      "4835/4835 [==============================] - 63s 13ms/step - loss: 2.8045e-04\n",
      "Epoch 99/100\n",
      "4835/4835 [==============================] - 50s 10ms/step - loss: 2.8712e-04\n",
      "Epoch 100/100\n",
      "4835/4835 [==============================] - 49s 10ms/step - loss: 2.8576e-04\n"
     ]
    }
   ],
   "source": [
    "n_input = 12\n",
    "n_features = 1\n",
    "generator = TimeseriesGenerator(train, train, length=n_input, batch_size=6)\n",
    "model = Sequential()\n",
    "model.add(LSTM(200, activation='relu', input_shape=(n_input, n_features)))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(1))\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "history = model.fit_generator(generator,epochs=100,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://a96f1843-e239-492b-973f-245403ea873c/assets\n"
     ]
    }
   ],
   "source": [
    "with open(\"model.pickle\", \"wb\") as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, None, 10) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 10), dtype=tf.float32, name='lstm_1_input'), name='lstm_1_input', description=\"created by layer 'lstm_1_input'\"), but it was called on an input with incompatible shape (None, 100, 1).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/prozapple/.pyenv/versions/anaconda3-2021.11/envs/project3-model/lib/python3.10/site-packages/keras/engine/training.py\", line 1845, in predict_function  *\n        return step_function(self, iterator)\n    File \"/Users/prozapple/.pyenv/versions/anaconda3-2021.11/envs/project3-model/lib/python3.10/site-packages/keras/engine/training.py\", line 1834, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/prozapple/.pyenv/versions/anaconda3-2021.11/envs/project3-model/lib/python3.10/site-packages/keras/engine/training.py\", line 1823, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/Users/prozapple/.pyenv/versions/anaconda3-2021.11/envs/project3-model/lib/python3.10/site-packages/keras/engine/training.py\", line 1791, in predict_step\n        return self(x, training=False)\n    File \"/Users/prozapple/.pyenv/versions/anaconda3-2021.11/envs/project3-model/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/prozapple/.pyenv/versions/anaconda3-2021.11/envs/project3-model/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Exception encountered when calling layer \"sequential_1\" (type Sequential).\n    \n    Input 0 of layer \"lstm_1\" is incompatible with the layer: expected shape=(None, None, 10), found shape=(None, 100, 1)\n    \n    Call arguments received by layer \"sequential_1\" (type Sequential):\n      • inputs=tf.Tensor(shape=(None, 100, 1), dtype=float32)\n      • training=False\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/prozapple/무현/Git/crypto-currency-prediction/model.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/prozapple/%E1%84%86%E1%85%AE%E1%84%92%E1%85%A7%E1%86%AB/Git/crypto-currency-prediction/model.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m batch \u001b[39m=\u001b[39m train[\u001b[39m-\u001b[39mn_input:]\u001b[39m.\u001b[39mreshape((\u001b[39m1\u001b[39m, n_input, n_features))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/prozapple/%E1%84%86%E1%85%AE%E1%84%92%E1%85%A7%E1%86%AB/Git/crypto-currency-prediction/model.ipynb#X34sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_input):   \n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/prozapple/%E1%84%86%E1%85%AE%E1%84%92%E1%85%A7%E1%86%AB/Git/crypto-currency-prediction/model.ipynb#X34sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     pred_list\u001b[39m.\u001b[39mappend(model\u001b[39m.\u001b[39;49mpredict(batch)[\u001b[39m0\u001b[39m]) \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/prozapple/%E1%84%86%E1%85%AE%E1%84%92%E1%85%A7%E1%86%AB/Git/crypto-currency-prediction/model.ipynb#X34sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     batch \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(batch[:,\u001b[39m1\u001b[39m:,:],[[pred_list[i]]],axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/prozapple/%E1%84%86%E1%85%AE%E1%84%92%E1%85%A7%E1%86%AB/Git/crypto-currency-prediction/model.ipynb#X34sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m add_dates \u001b[39m=\u001b[39m [train_dates\u001b[39m.\u001b[39mindex[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m DateOffset(hours\u001b[39m=\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39m20\u001b[39m) ]\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2021.11/envs/project3-model/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/dw/94thms1n5817tsrqwqcs_3rr0000gn/T/__autograph_generated_filefgug9y6i.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/prozapple/.pyenv/versions/anaconda3-2021.11/envs/project3-model/lib/python3.10/site-packages/keras/engine/training.py\", line 1845, in predict_function  *\n        return step_function(self, iterator)\n    File \"/Users/prozapple/.pyenv/versions/anaconda3-2021.11/envs/project3-model/lib/python3.10/site-packages/keras/engine/training.py\", line 1834, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/prozapple/.pyenv/versions/anaconda3-2021.11/envs/project3-model/lib/python3.10/site-packages/keras/engine/training.py\", line 1823, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/Users/prozapple/.pyenv/versions/anaconda3-2021.11/envs/project3-model/lib/python3.10/site-packages/keras/engine/training.py\", line 1791, in predict_step\n        return self(x, training=False)\n    File \"/Users/prozapple/.pyenv/versions/anaconda3-2021.11/envs/project3-model/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/prozapple/.pyenv/versions/anaconda3-2021.11/envs/project3-model/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Exception encountered when calling layer \"sequential_1\" (type Sequential).\n    \n    Input 0 of layer \"lstm_1\" is incompatible with the layer: expected shape=(None, None, 10), found shape=(None, 100, 1)\n    \n    Call arguments received by layer \"sequential_1\" (type Sequential):\n      • inputs=tf.Tensor(shape=(None, 100, 1), dtype=float32)\n      • training=False\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "pred_list = []\n",
    "n_features = 1\n",
    "n_input = 100\n",
    "batch = train[-n_input:].reshape((1, n_input, n_features))\n",
    "\n",
    "for i in range(n_input):   \n",
    "    pred_list.append(model.predict(batch)[0]) \n",
    "    batch = np.append(batch[:,1:,:],[[pred_list[i]]],axis=1)\n",
    "add_dates = [train_dates.index[-1] + DateOffset(hours=x) for x in range(0,20) ]\n",
    "future_dates = pd.DataFrame(index=add_dates[1:],columns=train_dates.columns)\n",
    "df_predict = pd.DataFrame(scaler.inverse_transform(pred_list),\n",
    "                          index=future_dates[-n_input:].index, columns=['Prediction'])\n",
    "\n",
    "df_proj = pd.concat([train_dates,df_predict], axis=1)\n",
    "\n",
    "df_proj.tail(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = []\n",
    "trainY = []\n",
    "\n",
    "n_future = 100\n",
    "n_past = 1000 \n",
    "\n",
    "for i in range(n_past, len(train_transformed) - n_future +1):\n",
    "    trainX.append(train_transformed[i - n_past:i, 0:train_transformed.shape[1]])\n",
    "    trainY.append(train_transformed[i + n_future - 1:i + n_future, 0])\n",
    "\n",
    "trainX, trainY = np.array(trainX), np.array(trainY)\n",
    "\n",
    "print('trainX shape == {}.'.format(trainX.shape))\n",
    "print('trainY shape == {}.'.format(trainY.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=True))\n",
    "model.add(LSTM(32, activation='relu', return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(trainY.shape[1]))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(trainX, trainY, epochs=5, batch_size=16, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"2layer.pickle\", \"rb\") as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "n_future = 90\n",
    "forecast_period_dates = pd.date_range(list(train_dates)[-1], periods=n_future, freq=\"1h\").tolist()\n",
    "forecast = model.predict(trainX[-n_future:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_future = scaler.inverse_transform(forecast)\n",
    "y_pred_future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "# testY = scaler.inverse_transform([testY])\n",
    "\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('project3-model')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08729205d2747d7bbb5a1998b41f3850577c648c718106806e890c20736aa893"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
